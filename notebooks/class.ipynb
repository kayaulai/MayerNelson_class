{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mayer & Nelson (2020) Phonotactic learning with neural language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get the data into Python.\n",
    "\n",
    "The raw data looks like this:\n",
    "\n",
    "    a a k k o n e n\n",
    "    a a k k o s e l l i n e n\n",
    "    a a k k o s e l l i s e s t i\n",
    "    a a k k o s e l l i s u u s\n",
    "    a a k k o s i t t a i n\n",
    "    a a k k o s j ae r j e s t y s\n",
    "    a a k k o s n u m e e r i n e n\n",
    "    a a k k o s t a a\n",
    "\n",
    "Each line is a word and each phoneme is separated by spaces.\n",
    "\n",
    "The function `get_corpus_data()` takes an input file, and turns it into a list of lists, with each item of the larger list being a list of phonemes. Note that `str.rstrip()` removes **trailing** space characters, i.e at the end, but not those characters at the beginning. So the line breaks at the end of each line are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_corpus_data(filename):\n",
    "    \"\"\"Reads input file and coverts it to list of lists, adding word boundary \n",
    "    markers <s> (start) and <e> (end).\n",
    "    \"\"\"\n",
    "    raw_data = []\n",
    "    file = open(filename, 'r', encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        line = ['<s>'] + line.split(' ') + ['<e>']\n",
    "        raw_data.append(line)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_finnish = get_corpus_data(\"../sample_data/corpora/finnish_training.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function, `process_data()`, takes the data we just got, and turns it into a dataset through the following steps:\n",
    "1. The data is shuffled\n",
    "2. Each word is padded to the maximum length with the padding character `<p>` so that each data point has identical length\n",
    "3. Gets the inventory, i.e. every possible phone in the data, with `<p>` at index 0\n",
    "4. Assigns indices to the phones, and provides dictionaries to convert from one to the other\n",
    "5. Creates the training and dev sets for PyTorch using `torch.LongTensor()`, a tensor data type that stores tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(string_training_data, dev=True, training_split=60):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        string_training_data (list(list(chr))): A list where each item\n",
    "                                                is a list of phones\n",
    "                                                from a word.\n",
    "        dev (bool, optional): Whether we want to split out a dev set.\n",
    "                              Defaults to True.\n",
    "        training_split (int, optional): Percentage of data used for\n",
    "                                        training. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing five items: the 'vocabulary' (i.e.\n",
    "               phone inventory), a dictionary for looking up indices\n",
    "               from phones, a dictionary for looking up phones from\n",
    "               indices, the training set, and the development set.\n",
    "    \"\"\"\n",
    "\n",
    "    random.shuffle(string_training_data)\n",
    "    # all data points need to be padded to the maximum length\n",
    "    max_chars = max([len(x) for x in string_training_data])\n",
    "    string_training_data = [\n",
    "        sequence + ['<p>'] * (max_chars - len(sequence)) \n",
    "        for sequence in string_training_data]\n",
    "    # get the inventory and build both directions of dicts  \n",
    "    # this will store the set of possible phones\n",
    "    inventory = list(set(phone for word in string_training_data for phone in word))\n",
    "    inventory = ['<p>'] + [x for x in inventory if x != '<p>'] #ensure that the padding symbol is at index 0\n",
    "\n",
    "    # dictionaries for looking up the index of a phone and vice versa\n",
    "    phone2ix = {p: ix for (ix, p) in enumerate(inventory)}\n",
    "    ix2phone = {ix: p for (ix, p) in enumerate(inventory)}\n",
    "\n",
    "    as_ixs = [\n",
    "        torch.LongTensor([phone2ix[p] for p in sequence])\n",
    "        for sequence in string_training_data\n",
    "      ]\n",
    "\n",
    "    if not dev:\n",
    "        training_data = torch.stack(as_ixs, 0)\n",
    "        # simpler make a meaningless tiny dev than to have a different eval \n",
    "        # training method that doesn't compute Dev perplexity\n",
    "        dev = torch.stack(as_ixs[-10:], 0)\n",
    "    else:\n",
    "        split = int(len(as_ixs) * (training_split/100))\n",
    "        training_data = torch.stack(as_ixs[:split], 0)\n",
    "        dev = torch.stack(as_ixs[split:], 0)\n",
    "\n",
    "    return (inventory, phone2ix, ix2phone, training_data, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "inventory, phone2ix, ix2phone, training, dev = process_data(\n",
    "        raw_data_finnish, dev=True, training_split=60\n",
    "    )\n",
    "inventory_size = len(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  2, 16,  ...,  0,  0,  0],\n",
      "        [ 5, 12, 19,  ...,  0,  0,  0],\n",
      "        [ 5,  4, 23,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 5,  4, 23,  ...,  0,  0,  0],\n",
      "        [ 5, 16,  2,  ...,  0,  0,  0],\n",
      "        [ 5,  4, 13,  ...,  0,  0,  0]])\n",
      "torch.Size([56292, 32])\n",
      "tensor([[ 5, 10, 22,  ...,  0,  0,  0],\n",
      "        [ 5, 18,  2,  ...,  0,  0,  0],\n",
      "        [ 5,  2, 18,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 5, 15, 22,  ...,  0,  0,  0],\n",
      "        [ 5,  2, 13,  ...,  0,  0,  0],\n",
      "        [ 5, 21, 12,  ...,  0,  0,  0]])\n",
      "torch.Size([37529, 32])\n",
      "torch.Size([10, 32])\n",
      "tensor([ 2, 16, 18, 17, 10, 10,  7,  8, 22, 21, 21,  7, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 19,\n",
      "         3, 23, 17,  2, 10, 22, 10, 23,  8, 23, 19, 10, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4, 23, 21, 21,\n",
      "         7, 10, 10, 23, 21, 11,  2, 22, 19, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  6, 22, 19, 15, 23,\n",
      "        21,  7, 12,  8,  7, 15,  7, 17, 14, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6, 22, 21, 10, 12, 12, 10, 10,\n",
      "        22, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  4, 10, 23, 19, 13,  7,  2, 15, 18,\n",
      "        12, 21, 12, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19, 18, 14, 22, 25,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16, 12,  4, 12,  8, 22, 10, 18, 19, 25,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17,  7, 19, 19, 12,  2, 10,  7, 19, 25,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 12, 15, 22, 19, 10, 23, 17, 13, 25,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([[ 2, 16, 18, 17, 10, 10,  7,  8, 22, 21, 21,  7, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 19,  3, 23, 17,  2, 10, 22, 10, 23,  8, 23, 19, 10, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 23, 21, 21,  7, 10, 10, 23, 21, 11,  2, 22, 19, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [23,  6, 22, 19, 15, 23, 21,  7, 12,  8,  7, 15,  7, 17, 14, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 22, 21, 10, 12, 12, 10, 10, 22, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11,  4, 10, 23, 19, 13,  7,  2, 15, 18, 12, 21, 12, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 18, 14, 22, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [16, 12,  4, 12,  8, 22, 10, 18, 19, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [17,  7, 19, 19, 12,  2, 10,  7, 19, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 12, 15, 22, 19, 10, 23, 17, 13, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "torch.Size([10, 32, 26])\n",
      "tensor([[[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1348,  0.1234,  0.3470,  ..., -0.1323, -0.0559, -0.3883],\n",
      "         [ 0.1131, -0.0695,  0.3031,  ...,  0.0245, -0.1391, -0.2129],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.2363,  0.1325,  0.1683,  ...,  0.0086, -0.1846, -0.0095],\n",
      "         [ 0.1430,  0.3388, -0.1756,  ...,  0.0010, -0.1351, -0.0618],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1006,  0.3132, -0.0226,  ..., -0.3514, -0.1742, -0.0527],\n",
      "         [ 0.4475,  0.2166,  0.1853,  ..., -0.0256,  0.1947, -0.3491],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.2562, -0.0902,  0.3308,  ..., -0.2798,  0.0138, -0.3897],\n",
      "         [ 0.3075,  0.2420, -0.0485,  ...,  0.1512, -0.2378,  0.2994],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [-0.0674,  0.4198, -0.2527,  ..., -0.3878,  0.3840, -0.5290],\n",
      "         [ 0.3382,  0.4207,  0.2537,  ..., -0.4789,  0.5245,  0.1056],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1348,  0.1234,  0.3470,  ..., -0.1323, -0.0559, -0.3883],\n",
      "         [ 0.0683,  0.1408,  0.0429,  ...,  0.2561, -0.4034,  0.1647],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1348,  0.1234,  0.3470,  ..., -0.1323, -0.0559, -0.3883],\n",
      "         [ 0.1131, -0.0695,  0.3031,  ...,  0.0245, -0.1391, -0.2129],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.2363,  0.1325,  0.1683,  ...,  0.0086, -0.1846, -0.0095],\n",
      "         [ 0.1430,  0.3388, -0.1756,  ...,  0.0010, -0.1351, -0.0618],\n",
      "         ...,\n",
      "         [ 0.1277, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1006,  0.3132, -0.0226,  ..., -0.3514, -0.1742, -0.0527],\n",
      "         [ 0.4475,  0.2166,  0.1853,  ..., -0.0256,  0.1947, -0.3491],\n",
      "         ...,\n",
      "         [ 0.1277, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.2562, -0.0902,  0.3308,  ..., -0.2798,  0.0138, -0.3897],\n",
      "         [ 0.3075,  0.2420, -0.0485,  ...,  0.1512, -0.2378,  0.2994],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [-0.0674,  0.4198, -0.2527,  ..., -0.3878,  0.3840, -0.5290],\n",
      "         [ 0.3382,  0.4207,  0.2537,  ..., -0.4789,  0.5245,  0.1056],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1348,  0.1234,  0.3470,  ..., -0.1323, -0.0559, -0.3883],\n",
      "         [ 0.0683,  0.1408,  0.0429,  ...,  0.2561, -0.4034,  0.1647],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(training)\n",
    "print(training.size())\n",
    "print(dev)\n",
    "print(dev.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model structure that we'll use:\n",
    "- The `Emb_RNNLM` class inherits from `nn.Module`, which is the most basic neural network class in Python\n",
    "- The constructor function of `Emb_RNNLM()` takes a dictionary called `params` that contains the following info:\n",
    "    - `inv_size`: Size of the phone inventory, i.e. vocabulary\n",
    "    - `d_emb`: Number of dimensions in the embedding\n",
    "    - `n_layers`: Number of layers in the model\n",
    "    - `d_hid`: Number of dimensions in hidden layer\n",
    "- The embedding layer and the recurrent layer are defined with respect to these parameters:\n",
    "    - The projection layer (`nn.Embedding`)'s constructor function takes the vocabulary size and the dimensionality of the embedding\n",
    "      as parameters.\n",
    "      - Question: What are the dimensions of its input and output? What is the activation function?\n",
    "    - The recurrent layer (`nn.RNN`) implements a simple recurrent network (SRN), a.k.a. an Elman network\n",
    "      - By default, tanh is used as the activation function\n",
    "        - Questions: What are the input and output dimensionality?\n",
    "      - Multiple hidden layers are possible, but for this paper (and this week) we'll just do one\n",
    "      - `batch_first=True` tells PyTorch that the input passed to this layer will consist of the embeddings first, then the sequence lengths.\n",
    "    - The output layer (`nn.Linear`) outputs a score for each phone, the softmax of which is the probability of the next phoneme\n",
    "      - If the projection and output layers have the same dimension, then the model will make their weights identical.\n",
    "        - Question: How does this affect model variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Emb_RNNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Emb_RNNLM, self).__init__()\n",
    "        self.vocab_size = params['inv_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.n_layers = params['num_layers']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        \n",
    "        # input to recurrent layer, default nonlinearity is tanh\n",
    "        self.i2R = nn.RNN(\n",
    "            self.d_emb, self.d_hid, batch_first=True, num_layers=self.n_layers\n",
    "        )\n",
    "        # recurrent to output layer\n",
    "        self.R2o = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        if params['tied']:\n",
    "            if self.d_emb == self.d_hid:\n",
    "                self.R2o.weight = self.embeddings.weight\n",
    "            else:\n",
    "                print(\"Dimensions don't support tied embeddings\")\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batches, seq_len = batch.size()\n",
    "        embs = self.embeddings(batch)\n",
    "        output, hidden = self.i2R(embs)\n",
    "        outputs = self.R2o(output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions don't support tied embeddings\n"
     ]
    }
   ],
   "source": [
    "rnn_params = {}\n",
    "rnn_params['d_emb'] = 24\n",
    "rnn_params['d_hid'] = 64\n",
    "rnn_params['num_layers'] = 1\n",
    "rnn_params['batch_size'] = 64\n",
    "rnn_params['learning_rate'] = .005\n",
    "rnn_params['epochs'] = 10\n",
    "rnn_params['tied'] = True\n",
    "rnn_params['inv_size'] = inventory_size\n",
    "RNN = Emb_RNNLM(rnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.2363,  0.1325,  0.1683,  ...,  0.0086, -0.1846, -0.0095],\n",
      "         [ 0.1430,  0.3388, -0.1756,  ...,  0.0010, -0.1351, -0.0618],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1006,  0.3132, -0.0226,  ..., -0.3514, -0.1742, -0.0527],\n",
      "         [ 0.4475,  0.2166,  0.1853,  ..., -0.0256,  0.1947, -0.3491],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.3557,  0.3604,  0.2864,  ..., -0.0796,  0.1787, -0.6470],\n",
      "         [ 0.3669,  0.1937,  0.2626,  ..., -0.1232, -0.1413, -0.1795],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1292, -0.3733,  0.4396],\n",
      "         [ 0.1277, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.2562, -0.0902,  0.3308,  ..., -0.2798,  0.0138, -0.3897],\n",
      "         [ 0.3075,  0.2420, -0.0485,  ...,  0.1512, -0.2378,  0.2994],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [-0.0674,  0.4198, -0.2527,  ..., -0.3878,  0.3840, -0.5290],\n",
      "         [ 0.3382,  0.4207,  0.2537,  ..., -0.4789,  0.5245,  0.1056],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]],\n",
      "\n",
      "        [[ 0.2000, -0.0308,  0.0767,  ..., -0.0590,  0.2589,  0.1230],\n",
      "         [ 0.1348,  0.1234,  0.3470,  ..., -0.1323, -0.0559, -0.3883],\n",
      "         [ 0.0683,  0.1408,  0.0429,  ...,  0.2561, -0.4034,  0.1647],\n",
      "         ...,\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396],\n",
      "         [ 0.1276, -0.4260, -0.0736,  ..., -0.1291, -0.3733,  0.4396]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(RNN(training[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the details for the function for model training:\n",
    "- The cross entropy loss is equivalent to the negative log-likelihood loss,\n",
    "- The Adams optimiser is used for performing gradient descent. It is a slightly\n",
    "  modified version of gradient descent, which fiddles around with the learning rate\n",
    "  and adds a feature called momentum to make the model learn faster.\n",
    "- As with Assignment 1, the data is divided into a bunch of batches controlled by \n",
    "  batch size. The `batches` variable stores the first and last position of within\n",
    "  the training set of each batch.\n",
    "- Within each epoch:\n",
    "  - Each epoch shuffles the order of the batches\n",
    "  - Within each batch:\n",
    "    - Predictions are calculated:\n",
    "      - `preds` houses the predicted scores (not probabilities!)\n",
    "      - `targets` houses the predictions\n",
    "      - `criterion` calculates the loss, taking `preds` and `targets` as input\n",
    "    - A backward pass is performed\n",
    "    - Weights are updated `Optimizer.step()`\n",
    "    - Gradients are cleared\n",
    "    - Loss for the epoch is updated\n",
    "  - Perplexity is calculated at the end of each epoch\n",
    "  - If, at the end of the epoch, dev perplexity hasn't moved by more than .01,\n",
    "    then we end training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def train_lm(dataset, dev, params, net):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    (num_examples, seq_len) = dataset.size()    \n",
    "    batches = [\n",
    "        (start, start + params['batch_size']) \n",
    "        for start in range(0, num_examples, params['batch_size'])\n",
    "    ]\n",
    "    \n",
    "    prev_perplexity = 1e10\n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        random.shuffle(batches)\n",
    "        \n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "            batch = dataset[start:end]\n",
    "            preds = net(batch)\n",
    "            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss.detach()\n",
    "        dev_perplexity = compute_perplexity(dev, net)\n",
    "\n",
    "        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev perplexity: %0.2f' %\n",
    "              (epoch, ep_loss, time.time()-start_time, dev_perplexity))\n",
    "        # stop early criterion, increasing perplexity on dev \n",
    "        if dev_perplexity - prev_perplexity > 0.01:\n",
    "            print('Stop early reached')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity is not automatically given, so we do need to write a function for it ourselves. Fortunately, there's an easy way to calculate it from the loss!\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\rho &= \\exp \\left(-\\frac{1}{n_{tok}} \\sum_{i = 1}^{n_{tok}} \\log(\\Pr(\\text{phoneme}_i|\\text{prev tokens})) \\right)\\\\\n",
    "&= \\exp \\left(\\frac{1}{n_{tok}} L(\\text{all tokens}) \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "(Incidentally, there is some repeated code between this function and the previous; you could potentially think of a way to make a third function to avoid this repetition!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(dataset, net, bsz=64):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    num_examples, seq_len = dataset.size()\n",
    "    \n",
    "    batches = [(start, start + bsz) for start in\\\n",
    "               range(0, num_examples, bsz)]\n",
    "    \n",
    "    total_unmasked_tokens = 0.\n",
    "    nll = 0.\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "        batch = dataset[start:end]\n",
    "        ut = torch.nonzero(batch).size(0)\n",
    "        preds = net(batch)\n",
    "        targets = batch[:, 1:].contiguous().view(-1)\n",
    "        preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "        loss = criterion(preds, targets)\n",
    "        nll += loss.detach()\n",
    "        total_unmasked_tokens += ut\n",
    "\n",
    "    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n",
    "    return perplexity.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1943.01, time: 6.54 sec, dev perplexity: 6.99\n",
      "epoch: 1, loss: 1835.14, time: 6.28 sec, dev perplexity: 6.76\n",
      "epoch: 2, loss: 1808.34, time: 6.04 sec, dev perplexity: 6.65\n",
      "epoch: 3, loss: 1794.52, time: 6.06 sec, dev perplexity: 6.55\n",
      "epoch: 4, loss: 1785.98, time: 6.52 sec, dev perplexity: 6.53\n",
      "epoch: 5, loss: 1780.50, time: 6.83 sec, dev perplexity: 6.48\n",
      "epoch: 6, loss: 1776.29, time: 6.97 sec, dev perplexity: 6.44\n",
      "epoch: 7, loss: 1773.24, time: 7.47 sec, dev perplexity: 6.48\n",
      "epoch: 8, loss: 1770.87, time: 9.38 sec, dev perplexity: 6.44\n",
      "epoch: 9, loss: 1768.70, time: 13.14 sec, dev perplexity: 6.42\n"
     ]
    }
   ],
   "source": [
    "train_lm(training, dev, rnn_params, RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we get to model evaluation. The idea is to compute perplexity using the training set.\n",
    "\n",
    "Because we want to compute perplexity for a single word each time, instead of an entire bunch of words, we only need to input a tensor representing a single word. `torch.unsqueeze()` adds an additional dimension so that the input is of the correct shape for `compute_perplexity()`, which assumes there are multiple words.\n",
    "\n",
    "(Again, there is some repetition in code with some functions above - think about how to get rid of it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_probs(input_file, model, phone2ix, out_filename):\n",
    "    inp_file = open(input_file, 'r', encoding='UTF-8')\n",
    "    out_file = open(out_filename,'w', encoding='UTF-8')\n",
    "    data_tens = []\n",
    "    as_strings = []\n",
    "    for line in inp_file:\n",
    "        line = line.rstrip()\n",
    "        as_strings.append(line.replace(' ',''))\n",
    "        line = line.split(' ')\n",
    "        line = ['<s>'] + line + ['<e>']\n",
    "        line_as_tensor = torch.LongTensor([phone2ix[p] for p in line])\n",
    "        data_tens.append(line_as_tensor)\n",
    "\n",
    "    num_points = len(data_tens)\n",
    "\n",
    "    for i,word in enumerate(data_tens):\n",
    "        curr_string = as_strings[i]\n",
    "        out_file.write(curr_string + '\\t' + str(compute_perplexity(word.unsqueeze(0), model).numpy()) + '\\n')\n",
    "    \n",
    "    inp_file.close()\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `RNN.eval()` tells PyTorch that we're going to use the data to be input into the neural network for evaluation only. (This doesn't have huge impacts on this particular case, but if your model contains things like batch normalisation or dropout, which we haven't discussed, this is an essential step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "RNN.eval()\n",
    "get_probs(\"../sample_data/test_data/finnish_test.txt\",\n",
    "    RNN, phone2ix,\n",
    "    \"../output/finnish.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the results. What do you notice?\n",
    "\n",
    "    poemivo\t\t15.390921\n",
    "    toemagu\t\t20.85924\n",
    "    rytoky\t\t19.299955\n",
    "    jaebejaeli\t27.688951\n",
    "    \n",
    "\n",
    "    rasiketo\t7.663081\n",
    "    hahohasu\t12.35365\n",
    "    nypimide\t8.837763\n",
    "    rojapotto\t8.841386\n",
    "    kentittoe\t9.206199\n",
    "    helesa\t\t9.307686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The other two case studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new function that includes everything we did above, but extensible to the other two case studies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def run_full_pipeline(train_path, test_path, output_path):\n",
    "    #Prepare data\n",
    "    raw_data = get_corpus_data(train_path)\n",
    "    inventory, phone2ix, ix2phone, training, dev = process_data(\n",
    "        raw_data, dev=True, training_split=60\n",
    "    )\n",
    "    inventory_size = len(inventory)\n",
    "\n",
    "    #Set hyperparameters and model structure\n",
    "    rnn_params = {}\n",
    "    rnn_params['d_emb'] = 24\n",
    "    rnn_params['d_hid'] = 64\n",
    "    rnn_params['num_layers'] = 1\n",
    "    rnn_params['batch_size'] = 64\n",
    "    rnn_params['learning_rate'] = .005\n",
    "    rnn_params['epochs'] = 10\n",
    "    rnn_params['tied'] = True\n",
    "    rnn_params['inv_size'] = inventory_size\n",
    "    RNN = Emb_RNNLM(rnn_params)\n",
    "\n",
    "    #Train model\n",
    "    train_lm(training, dev, rnn_params, RNN)\n",
    "\n",
    "    #Evaluate model\n",
    "    RNN.eval()\n",
    "    get_probs(test_path,\n",
    "        RNN, phone2ix,\n",
    "        output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model on the Cochabamba Quechua data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions don't support tied embeddings\n",
      "epoch: 0, loss: 61.01, time: 0.12 sec, dev perplexity: 5.84\n",
      "epoch: 1, loss: 47.82, time: 0.12 sec, dev perplexity: 5.20\n",
      "epoch: 2, loss: 46.08, time: 0.12 sec, dev perplexity: 5.10\n",
      "epoch: 3, loss: 45.43, time: 0.11 sec, dev perplexity: 5.04\n",
      "epoch: 4, loss: 44.85, time: 0.12 sec, dev perplexity: 4.98\n",
      "epoch: 5, loss: 44.46, time: 0.12 sec, dev perplexity: 4.94\n",
      "epoch: 6, loss: 43.97, time: 0.11 sec, dev perplexity: 4.91\n",
      "epoch: 7, loss: 43.64, time: 0.11 sec, dev perplexity: 4.90\n",
      "epoch: 8, loss: 43.42, time: 0.13 sec, dev perplexity: 4.89\n",
      "epoch: 9, loss: 43.15, time: 0.13 sec, dev perplexity: 4.88\n"
     ]
    }
   ],
   "source": [
    "run_full_pipeline(\"../sample_data/corpora/quechua_training.txt\",\n",
    "    \"../sample_data/test_data/quechua_test.txt\",\n",
    "    \"../output/quechua.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice this time? (`+` indicates ejective.)\n",
    "\n",
    "    wap+a\t4.8824334\n",
    "    Last+i\t5.0887637\n",
    "    p+unsi\t6.026197\n",
    "    p+awi\t4.6279793\n",
    "\n",
    "    kiLc+u\t8.266438\n",
    "    kajc+u\t10.616985\n",
    "    kap+a\t7.2731633\n",
    "    qajc+i\t9.161771\n",
    "    qajc+u\t9.812296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the last English case study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions don't support tied embeddings\n",
      "epoch: 0, loss: 3074.04, time: 13.04 sec, dev perplexity: 8.21\n",
      "epoch: 1, loss: 2972.34, time: 13.65 sec, dev perplexity: 8.09\n",
      "epoch: 2, loss: 2953.39, time: 18.52 sec, dev perplexity: 8.01\n",
      "epoch: 3, loss: 2943.21, time: 17.97 sec, dev perplexity: 7.97\n",
      "epoch: 4, loss: 2936.54, time: 15.58 sec, dev perplexity: 7.95\n",
      "epoch: 5, loss: 2932.39, time: 14.62 sec, dev perplexity: 7.93\n",
      "epoch: 6, loss: 2928.44, time: 14.61 sec, dev perplexity: 7.93\n",
      "epoch: 7, loss: 2925.80, time: 14.12 sec, dev perplexity: 7.91\n",
      "epoch: 8, loss: 2923.28, time: 14.00 sec, dev perplexity: 7.93\n",
      "epoch: 9, loss: 2921.79, time: 15.02 sec, dev perplexity: 7.91\n"
     ]
    }
   ],
   "source": [
    "run_full_pipeline(\"../sample_data/corpora/CMU_dict_IPA.txt\",\n",
    "    \"../sample_data/test_data/Daland_et_al_IPA.txt\",\n",
    "    \"../output/english.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice this time? (`+` indicates ejective.)\n",
    "\n",
    "Attested |Marginal |Unattested|\n",
    "|---|---|---|\n",
    "tw tɹ sw |gw ʃl |pw zɹ mɹ |\n",
    "ʃɹ pɹ pl |vw Sw |tl dn km |\n",
    "kw kɹ kl |ʃn ʃm |fn ml nl |\n",
    "gr gl fɹ |vl bw |dg pk lm|\n",
    "fl dɹ bɹ |dw fw |ln ɹl lt |\n",
    "bl sn sm |vɹ θw |ɹn ɹd rg|\n",
    "\n",
    "    bligɪf\t13.21389\n",
    "    blɛzɪg\t11.064883\n",
    "    dɹigɪf\t13.357114\n",
    "    glɛpɪd\t10.587212\n",
    "\n",
    "    gwibɪd\t8.843671\n",
    "    vwigɪf\t24.82807\n",
    "    ʃnigɪf\t19.082851\n",
    "    θwɛpɪd\t15.313112\n",
    "\n",
    "    dgɛpɪd\t22.96906\n",
    "    dnigɪf\t30.790936\n",
    "    lmɑtɪf\t22.789053\n",
    "    ɹlɛzɪg\t28.106743\n",
    "    ɹnɑsɪp\t24.705233"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
